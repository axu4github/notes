# 《Spark 快速大数据分析》读书笔记

## 第3章 RDD 编程

### 3.1 RDD 基础

> **在任何时候都可以进行重算是我们为什么把 `RDD` 描述为 `弹性` 的原因。** 当保存 `RDD` 数据的一台机器失败时， `Spark` 还可以以这种特性来重算出已经丢失的分区，这一过程对用户是完全透明的。
/.

- `Spark` 的 `RDD` 是一个 不可变的分布式对象集合。
- 每个 `RDD` 有多个分区，这些分区分布在集群中的不同节点上。
- `RDD` 有两种类型的操作： `Transformation` 和 `Action` 。
- `Transformation` 操作会把一个 `RDD` 转化成为一个新的 `RDD` 。
- `Action` 操作会通过 `RDD` 计算出一个结果，返回给 `Driver` 或者 把结构存储到外部的存储系统中。


### 3.2 创建 RDD

> 创建 `RDD` 有两种方法。

##### 读取外部数据集

```python
# 读取外部数据集创建 RDD
>>> lines = sc.textFile("/path/to/README.md")
```

##### 调用 parallelize() 函数

```python
# 调用 parallelize() 函数创建 RDD
>>> lines = sc.parallelize(["pandas", "i like pandas"])
```

### 3.3 RDD 操作

### 3.3.1 转化（Transformation）

- 转化（Transformation）操作是将一个 `RDD` 转化成另一个（新的）`RDD` ，而不是改变了原来的 `RDD` 。因为 `RDD` 是不可变的。
- `旧 RDD` 和 `新 RDD` 的关系被称为 `依赖（Dependencies）`。

### 3.3.2 行动（Ation）

- `collect()` 函数不能用在大规模数据集上，当存在大规模数据集应将它们写到外部存储系统中。
- 每当调用一次 `Action` 操作时，`RDD` 都会从头开始算一次，好的做法是将中间结果持久化（ `persist() / cache()` ）。

### 3.3.3 Lazy

- 所有的 `Transformation` 操作的都是 `Lazy` 的，即直到出现 `Action` 操作才会真正执行。这样做的好处是没有多余的中间结果，和更全面的优化。
- `Lazy` 是 `Spark` 比 `Hadoop` 快的主要特点之一。
