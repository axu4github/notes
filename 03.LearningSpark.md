# 《Spark 快速大数据分析》读书笔记

![](assets/markdown-img-paste-2017092122555322.png)

## 第3章 RDD 编程

### 3.1 RDD 基础

> **在任何时候都可以进行重算是我们为什么把 `RDD` 描述为 `弹性` 的原因。** 当保存 `RDD` 数据的一台机器失败时， `Spark` 还可以以这种特性来重算出已经丢失的分区，这一过程对用户是完全透明的。
/.

- `Spark` 的 `RDD` 是一个 不可变的分布式对象集合。
- 每个 `RDD` 有多个分区，这些分区分布在集群中的不同节点上。
- `RDD` 有两种类型的操作： `Transformation` 和 `Action` 。
- `Transformation` 操作会把一个 `RDD` 转化成为一个新的 `RDD` 。
- `Action` 操作会通过 `RDD` 计算出一个结果，返回给 `Driver` 或者 把结构存储到外部的存储系统中。


### 3.2 创建 RDD

> 创建 `RDD` 有两种方法。

##### 读取外部数据集

```python
# 读取外部数据集创建 RDD
>>> lines = sc.textFile("/path/to/README.md")
```

##### 调用 parallelize() 函数

```python
# 调用 parallelize() 函数创建 RDD
>>> lines = sc.parallelize(["pandas", "i like pandas"])
```

### 3.3 RDD 操作

#### 3.3.1 转化（Transformation）

- 转化（Transformation）操作是将一个 `RDD` 转化成另一个（新的）`RDD` ，而不是改变了原来的 `RDD` 。因为 `RDD` 是不可变的。
- `旧 RDD` 和 `新 RDD` 的关系被称为 `依赖（Dependencies）`。

#### 3.3.2 行动（Ation）

- `collect()` 函数不能用在大规模数据集上，当存在大规模数据集应将它们写到外部存储系统中。
- 每当调用一次 `Action` 操作时，`RDD` 都会从头开始算一次，好的做法是将中间结果持久化（ `persist() / cache()` ）。

#### 3.3.3 Lazy

- 所有的 `Transformation` 操作的都是 `Lazy` 的，即直到出现 `Action` 操作才会真正执行。这样做的好处是没有多余的中间结果，和更全面的优化。
- `Lazy` 是 `Spark` 比 `Hadoop` 快的主要特点之一。

### 3.4 向 Spark 传递函数

##### 错误样例

> 不要向 `Spark` 传递带有引用的函数，因为会将引用的所有内容都一并传递（有可能很大）。

```python
# 错误样例
class SearchFunctions(object):

    def __init__(self, query):
        self.query = query

    def isMatch(self, s):
        return self.query in s

    def getMatchesFunctionReference(self, rdd):
        # 问题：在 self.isMatch 中引用了整个 self。
        rdd.filter(self.isMatch)

    def getMatchesMemberReference(self, rdd):
        # 问题：在 self.query 中引用了整个 self。
        rdd.filter(lambda x: self.query in x)
```

##### 正确样例

> 解决方法，将需要传递的内容拿出来到一个局部变量中，再传递这个局部变量。

```python
# 正确样例
class WordFunctions(object):

    # ...

    def getMatchesNoReference(self, rdd):
        # 安全：只要把需要的字段提取到局部变量中。
        query = self.query
        return rdd.filter(lambda x: query in x)
```

### 3.6 持久化（缓存）

> 在 `org.apache.spark.storage.StorageLevel` 和 `pyspark.StorageLevel` 文件中描述了 `持久化级别` ，如果需要可以在 `持久化级别` 的末尾加上 `_2` 用来把持久化数据存储 `2` 份（如：`DISK_ONLY_2`）。

级别|使用空间|CPU时间|是否在内存中|是否在磁盘上|说明
-|-|-|-|-|-
`MEMORY_ONLY`|高|低|是|否|-
`MEMORY_ONLY_SER`|低|高|是|否|-
`MEMORY_AND_DISK`|高|中等|部分|部分|如果数据在内存中放不下，则溢写到磁盘上
`MEMORY_AND_DISK_SER`|低|高|部分|部分|如果数据在内存中放不下，则溢写到磁盘上。在内存中存放序列化后的数据
`DISK_ONLY`|低|高|否|是|-

> 如果要缓存的数据太多，内存放不下， `Spark` 会通过 `最近最少使用（LRU）` 策略把最老的分区从内存中移除。
>
> - 对于 `MEMORY_ONLY` 级别的缓存则直接被移除，若下次要访问则重新计算。
> - 对于 `MEMORY_AND_DISK` 级别的缓存则原存储在内存部分的缓存会被移动到磁盘上。

## 第4章 键值对操作

### 4.5 数据分区（进阶）

##### 场景

> 举个简单的例子，我们分析这样一个应用，它在内存中保存着一张很大的用户信息表--也就是一个由 `(UserID, UserInfo)` 对组成的 `RDD` ，其中 `UserInfo` 包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件--其实就是一个由 `(UserID, LinkInfo)` 对组成的表，存放着过去五分钟内某网站各用户的访问情况。

##### 需求

> 我们可能需要对用户访问其未订阅主题页面的情况进行统计。

##### 思路

> 我们可以使用 `Spark` 的 `join()` 操作来实现这个组合操作，其中需要把 `UserInfo` 和 `LinkInfo` 的有序对根据 `UserID` 进行分组。

##### 简单实现（未优化）

```scala
// 初始化代码，从 HDFS 上的一个 Hadoop SequenceFile 中读取用户信息
// userData 中的元素会根绝它们被读取时的来源（HDFS块所在的节点）来分布
// Spark 此时无法获知某个特定的 UserID 对应的记录位于哪个节点上
val sc = new SparkContext(...)
val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...").persist()

// 周期性调用函数来处理过去五分钟产生的事件日志
// 假设这是一个包含 (UserID, LinkInfo) 对的 SequenceFile
def processNewLogs(logFileName: String) {
    val events = sc.sequenceFile[UserID, LinkInfo](logFileName)
    val joined = userData.join(events) // RDD 键值对 (UserID, (UserInfo, LinkInfo))
    val offTopicVisits = joined.filter {
        case (userId, (userInfo, linkInfo)) =>
            !userInfo.topics.contains(linkInfo.topic)
    }.count()
    println("Number of visits to non-subscribed topics: " + offTopicVisits)
}
```

##### 分析

> 这段代码可以正确运行，但是不够高效。这是因为在每次调用 `processNewLogs()` 时都会用到 `join()` 操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同额记录进行连接操作。因为 `userData` 表比每五分钟出现的访问日志表 `events` 要大得多，所以要浪费时间做很多额外工作：在每次调用时都对 `userData` 表进行哈希值计算和跨节点数据 `混洗（shuffled）` ，虽然这些数据从来都不会变化。

![](assets/markdown-img-paste-20170922233955491.png)
